---
title: "Download File"
author: "Chris Selig"
date: "`r Sys.Date()`"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(
    echo = FALSE,
    message = FALSE,
    warning = FALSE
    )
```

## Description
This file is used to download the traffic incidents and construction detours data from the YYC Open data portal, using the provided API url

traffic incidents url: https://data.calgary.ca/resource/35ra-9556.csv
construction detours: https://data.calgary.ca/resource/w8zq-79bq.csv

Traffic incidents are updated on the Open data portal every 10min
Construction detours are updated on the Open data portal every day at 3am and 3pm

### Load Packages

```{r loadPackages, echo=FALSE}
library(dplyr)
library(lubridate)
library(readr)
library(janitor)
library(arrow)
```

### Create Functions 

```{r createFunctions}
download_data <- function(url, destfile) {
    download.file(
        url = url, 
        destfile = destfile
        )
  }
```

### Set Variables

These variables are used to set the location of the data files and the url to download the data from.
Also included is where to store the data after data wrangling is complete

```{r setVariables, echo=FALSE}
traffic_incidents_url <- "https://data.calgary.ca/resource/35ra-9556.csv"
construction_detours_url <- "https://data.calgary.ca/resource/w8zq-79bq.csv"
output_data_location <- "00_data/"
```

### Download Data

```{r downloadData}
download_data(url = traffic_incidents_url, destfile = "00_data/traffic_incidents.csv")
download_data(url = construction_detours_url, destfile = "00_data/construction_detours.csv")
```

### Load Data

Read in the raw data from csv files. This data will be wrangled as the next step.

```{r load_data}
traffic_incidents_raw <- readr::read_csv("00_data/traffic_incidents.csv")
construction_detours_raw <- readr::read_csv("00_data/construction_detours.csv")
```

## Data Wrangling

Using the janitor package, clean the names of the columns and using the lubridate package, convert the date columns to date time format. This data wrangling is completed for each of the downloaded data sets.

```{r trafficIncidentsWrangling}
traffic_incidents <- traffic_incidents_raw |> 
    janitor::clean_names() |> 
    mutate(
    start_dt = lubridate::ymd_hms(start_dt,tz = "Canada/Mountain"),
    modified_dt = lubridate::ymd_hms(modified_dt,tz = "Canada/Mountain"),
        )
```

```{r constructionDetoursWrangling}
construction_detours <- construction_detours_raw |> 
    janitor::clean_names() |> 
    mutate(
    start_dt = lubridate::ymd_hms(start_dt,tz = "Canada/Mountain"),
    end_dt = lubridate::ymd_hms(end_dt,tz = "Canada/Mountain")
        )
```

# Write the Data to parquet files

First create the 00_data directory if it doesn't exist
```{r createOutputDirectory}
if (!dir.exists(output_data_location)) {
    dir.create(output_data_location)
}
```

Using the arrow package, write the data to parquet files in the 00_data directory. The data can then be picked up by the next step in the pipeline.

```{r}
arrow::write_parquet(traffic_incidents, paste0(output_data_location,"traffic_incidents.parquet"))
arrow::write_parquet(construction_detours, paste0(output_data_location,"construction_detours.parquet"))
```

